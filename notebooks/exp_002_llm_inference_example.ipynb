{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM Inference Example","metadata":{}},{"cell_type":"markdown","source":"| Model | Result | Status |\n| ----- | ------ | --- |\n| Mixtral Instruct | The GPU memory requirement exceeds Kaggle's limit. | üí• Kaboom!! |\n| Pegasus | All Okay! | ‚úÖ Good |\n| GPT 2 | At least there are no errors. üòê | ‚úÖ Good |\n| [LLaMA 3.2 (i.e. latest)](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf) | At least there are no errors. üòê | ‚úÖ Good |\n| [FLAN-T5](https://huggingface.co/collections/google/flan-t5-release-65005c39e3201fff885e22fb) | At least there are no errors. üòê | ‚úÖ Good |\n\n<!--\n‚úÖ Good\nüí• Kaboom!!\n‚è≥ In Progress...\nü§î Problematic\n-->","metadata":{}},{"cell_type":"code","source":"# !pip install bitsandbytes>=0.39.0 sacremoses==0.0.53 accelerate -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-29T12:23:23.730717Z","iopub.execute_input":"2024-11-29T12:23:23.731516Z","iopub.status.idle":"2024-11-29T12:23:23.734837Z","shell.execute_reply.started":"2024-11-29T12:23:23.731479Z","shell.execute_reply":"2024-11-29T12:23:23.733985Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nlogin(token=user_secrets.get_secret('HUGGINGFACE_TOKEN'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:20:39.851430Z","iopub.execute_input":"2024-11-29T14:20:39.851676Z","iopub.status.idle":"2024-11-29T14:20:40.538047Z","shell.execute_reply.started":"2024-11-29T14:20:39.851650Z","shell.execute_reply":"2024-11-29T14:20:40.536978Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    device = 'cuda:0'\nelse:\n    device = 'cpu'\n\nprint('Device:', device)\nprint('Device name:', torch.cuda.get_device_name(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:20:40.539786Z","iopub.execute_input":"2024-11-29T14:20:40.540164Z","iopub.status.idle":"2024-11-29T14:20:43.849998Z","shell.execute_reply.started":"2024-11-29T14:20:40.540124Z","shell.execute_reply":"2024-11-29T14:20:43.848815Z"}},"outputs":[{"name":"stdout","text":"Device: cuda:0\nDevice name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    PegasusForConditionalGeneration,\n    PegasusTokenizer,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    pipeline,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:38:23.504913Z","iopub.execute_input":"2024-11-29T14:38:23.505498Z","iopub.status.idle":"2024-11-29T14:38:23.515023Z","shell.execute_reply.started":"2024-11-29T14:38:23.505466Z","shell.execute_reply":"2024-11-29T14:38:23.514343Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## FLAN-T5","metadata":{}},{"cell_type":"code","source":"model_id = \"google/flan-t5-large\"\n\ntokenizer = T5Tokenizer.from_pretrained(model_id)\nmodel = T5ForConditionalGeneration.from_pretrained(\n    model_id, \n    torch_dtype=torch.float16, \n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:41:47.570699Z","iopub.execute_input":"2024-11-29T14:41:47.571396Z","iopub.status.idle":"2024-11-29T14:41:50.311214Z","shell.execute_reply.started":"2024-11-29T14:41:47.571363Z","shell.execute_reply":"2024-11-29T14:41:50.310347Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"input_text = \"Who are you?\"\n\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:49:19.973681Z","iopub.execute_input":"2024-11-29T14:49:19.974039Z","iopub.status.idle":"2024-11-29T14:49:20.166047Z","shell.execute_reply.started":"2024-11-29T14:49:19.974011Z","shell.execute_reply":"2024-11-29T14:49:20.165059Z"}},"outputs":[{"name":"stdout","text":"<pad> adam</s>\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"## LLaMA 3","metadata":{}},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-3B\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id, \n    model_kwargs={\n        \"torch_dtype\": torch.bfloat16,\n    },\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:26:07.658850Z","iopub.execute_input":"2024-11-29T14:26:07.659188Z","iopub.status.idle":"2024-11-29T14:26:12.002628Z","shell.execute_reply.started":"2024-11-29T14:26:07.659159Z","shell.execute_reply":"2024-11-29T14:26:12.001812Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a28a3fc831e4e28b21ea0336490e8e8"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"%%time\npipe_params = {\n    'max_new_tokens': 100,\n    'pad_token_id': pipe.tokenizer.eos_token_id,\n}\n\nprompt = '''Saigon is'''\n\ngen_text = pipe(prompt, **pipe_params)[0]['generated_text']\nprint(f'[TEXT STARTS]\\n{gen_text}\\n[TEXT ENDS]')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:33:41.506864Z","iopub.execute_input":"2024-11-29T14:33:41.507472Z","iopub.status.idle":"2024-11-29T14:33:45.238321Z","shell.execute_reply.started":"2024-11-29T14:33:41.507436Z","shell.execute_reply":"2024-11-29T14:33:45.237463Z"}},"outputs":[{"name":"stdout","text":"[TEXT STARTS]\nSaigon is a city of contrasts and contradictions. The city is full of contrasts. The contrasts are apparent in its people. Some are well dressed and others are not. Some are rich and others are poor. Some are educated and others are not. Some are religious and others are not. Some are conservative and others are liberal. Some are friendly and others are not. Some are polite and others are not. Some are friendly and others are not. Some are friendly and others are not. Some are friendly and others\n[TEXT ENDS]\nCPU times: user 3.71 s, sys: 19.8 ms, total: 3.73 s\nWall time: 3.73 s\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## GPT 2","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T13:20:25.824799Z","iopub.execute_input":"2024-11-29T13:20:25.825565Z","iopub.status.idle":"2024-11-29T13:20:32.307511Z","shell.execute_reply.started":"2024-11-29T13:20:25.825525Z","shell.execute_reply":"2024-11-29T13:20:32.306599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = \"This is how to fry an egg:\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n\ngen_tokens = model.generate(\n    input_ids,\n    do_sample=True,\n    temperature=0.9,\n    max_length=200,\n    num_return_sequences=1,\n    pad_token_id=tokenizer.eos_token_id,\n)\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint('[GENERATED TEXT BEGINS]')\nprint(gen_text)\nprint('[GENERATED TEXT ENDS]')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T13:38:24.107575Z","iopub.execute_input":"2024-11-29T13:38:24.108141Z","iopub.status.idle":"2024-11-29T13:38:25.779043Z","shell.execute_reply.started":"2024-11-29T13:38:24.108108Z","shell.execute_reply":"2024-11-29T13:38:25.777898Z"}},"outputs":[{"name":"stdout","text":"[GENERATED TEXT BEGINS]\nThis is how to fry an egg:\n\n1 egg = 3% flour.\n\n1 slice of butter = 1 tsp salt.\n\n1 egg = 1/2 tsp cinnamon.\n\nServe warm. It's great to try this recipe on hot days.\n\n*The flour will absorb the moisture you've absorbed through the cooking process, so be sure to cook to a lower temperature (it will not give off as much sugar or carbs as is recommended, and you'll likely burn your hand).\n\nFor more egg-flourless recipes, check out these popular low carb eggs:\n\nHere's a post about how to do your own low carb egg filling.\n\nFor egg replacement, you can use a traditional non-dairy yogurt or milk substitute, like cottage cheese or almond milk or yogurt cheese.\n\nFor the \"low\" egg replacement, you can use a low-fat sour cream substitute, a sour cream derivative or a regular low\n[GENERATED TEXT ENDS]\nCPU times: user 1.67 s, sys: 0 ns, total: 1.67 s\nWall time: 1.67 s\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## Pegasus","metadata":{}},{"cell_type":"code","source":"model_name = \"google/pegasus-xsum\"\n\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T12:24:24.595571Z","iopub.execute_input":"2024-11-29T12:24:24.596348Z","iopub.status.idle":"2024-11-29T12:24:45.650538Z","shell.execute_reply.started":"2024-11-29T12:24:24.596299Z","shell.execute_reply":"2024-11-29T12:24:45.649622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:  34%|###3      | 765M/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5224c27cc984480c9a9129bb4f82644b"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a18d5ed5248c43ffb8f5f24792791ca6"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"src_text = [\n    \"\"\"\n    PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions.\n    The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be\n    affected by the shutoffs which were expected to last through at least midday tomorrow.\n    \"\"\",\n]\n\nsrc_text = list(map(lambda s: ' '.join(s.split()), src_text))\nbatch = tokenizer(src_text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\ntranslated = model.generate(**batch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T12:25:56.478742Z","iopub.execute_input":"2024-11-29T12:25:56.479120Z","iopub.status.idle":"2024-11-29T12:25:57.193542Z","shell.execute_reply.started":"2024-11-29T12:25:56.479085Z","shell.execute_reply":"2024-11-29T12:25:57.192646Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n# assert (\n#     tgt_text[0]\n#     == \"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"\n# )\n\nprint(len(tgt_text))\nprint(tgt_text[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T12:27:12.120991Z","iopub.execute_input":"2024-11-29T12:27:12.121333Z","iopub.status.idle":"2024-11-29T12:27:12.131977Z","shell.execute_reply.started":"2024-11-29T12:27:12.121305Z","shell.execute_reply":"2024-11-29T12:27:12.130921Z"}},"outputs":[{"name":"stdout","text":"1\nCalifornia's largest electricity provider has turned off power to hundreds of thousands of customers.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Mixtral Instruct","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n\nmodel_params = {\n    'torch_dtype': torch.float16,\n    # 'use_flash_attention_2': True,\n    'device_map': 'auto',\n}\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, **model_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:01:23.735044Z","iopub.execute_input":"2024-11-29T11:01:23.735410Z","iopub.status.idle":"2024-11-29T11:41:47.229434Z","shell.execute_reply.started":"2024-11-29T11:01:23.735383Z","shell.execute_reply":"2024-11-29T11:41:47.228446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e372002dabd3473dbfc47ab985e49c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0549bb79de24458a9984e4b9c71d11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3de9974c24064fdd999a3d6c05052d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73608a232f9a4f409cdb8d8e2f8748b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e74548d945e4edebe9022af4a5f4b87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eacedc23a68147b5a7bdbadcaa7b79f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a00d6be9db45589ad315ac87b2cce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a864c44b96b4b098e5934af9ad3005a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4397f94611684bacab0782d4a9f14b9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44b784bd2224375bc2e13e2826fe4b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4fcf35a5dc94bbcac7c81fd6ad108c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8270ce16161d453b93a26032ba094e6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee21b1d832f04d66a5b0dc13baf38a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e049c484b7b0497db9a430b00af33a60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e200e833bfb415b9f0d10a780e458ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85800165a8fb4a1391c51ab774e3ab10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4bf460c505d4a96b93a533d960cf441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd747128e5e74e3f899fd91e7e63ef26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f0678f692aa4f3ab952c9c317592f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2283b6b95c84905b781b26ba53abae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d25ac899fa48d7993754e74723f591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd503f2f8534adbadb01bb98568899b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f777f7a14d242ee811987d5252857e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd30357658f146b3abf6b68532afbff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae67da2d635c451e935120e14ca05efe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f345b040a1c417bbcc2f9d38ca87153"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 469765632 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0483c8670140deb07b4304c187b3fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b2ffe76a344e9fb24009d5f3bac672"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 2min 37s, sys: 3min 10s, total: 5min 48s\nWall time: 40min 23s\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"üí•üí•üí• The Kaggle kernel crashed after the previous cell.\nSo, MixtralInstruct dismissed!","metadata":{}},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T11:58:19.382972Z","iopub.execute_input":"2024-11-29T11:58:19.383545Z","iopub.status.idle":"2024-11-29T11:58:19.421609Z","shell.execute_reply.started":"2024-11-29T11:58:19.383502Z","shell.execute_reply":"2024-11-29T11:58:19.420322Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/big_modeling.py:456\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."],"ename":"RuntimeError","evalue":"You can't move a model that has some modules offloaded to cpu or disk.","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"%%time\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nprint('Tokenizing input..')\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n\nprint('Generating output as embeddings..')\noutputs = model.generate(inputs, max_new_tokens=20)\n\nprint('‚úÖ All Done')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}